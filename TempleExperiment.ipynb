{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 0, 0, 0), 0, False)\n",
      " ---------\n",
      "|       * |\n",
      "|    >    |\n",
      "| -     + |\n",
      " ---------  memory: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# inspired by OpenAI's \"Frozen Lake\" gym https://github.com/openai/gym. Our interface will loosly follow gym.\n",
    "# and Arthur Juliani's Q-Learning tutorial https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "\n",
    "# signal rewuard issued once.\n",
    "\n",
    "class temple:\n",
    "    \n",
    "    # These are the spatial coordinates\n",
    "    # of the temple.\n",
    "    \n",
    "    xDim = 3\n",
    "    yDim = 3\n",
    "    \n",
    "    # Memory has three states.\n",
    "    # Always initialzed to zero.\n",
    "    # Agent manipulates as it sees fit\n",
    "    \n",
    "    memoryDim = 3\n",
    "    \n",
    "    # The visible signal will have three states:\n",
    "    #  0: not visible\n",
    "    #  1: signal indicates reward is at altar 1\n",
    "    #  2: signal indicates reward is at altar 2\n",
    "    \n",
    "    signalDim = 3\n",
    "    \n",
    "    altarOnePosition = (0,2)\n",
    "    altarTwoPosition = (2,2)\n",
    "    signalPosition   = (1,1)\n",
    "    \n",
    "    def __init__(self, altarReward = 1, altarPenalty = -1, signalReward = 0.2, memoryClearAvailable = False):\n",
    "        self.altarReward  = altarReward\n",
    "        self.altarPenalty = altarPenalty\n",
    "        self.signalReward = signalReward\n",
    "        self.memoryClearable = memoryClearAvailable        \n",
    "        self.observationSpace = [self.xDim, self.yDim, self.memoryDim, self.signalDim]\n",
    "        self.actionSpace      = [7 if self.memoryClearable else 6]\n",
    "\n",
    "    def getState(self):\n",
    "        return self.x, self.y, self.memory, self.signal if (self.x, self.y) == self.signalPosition else 0\n",
    "         \n",
    "    def reset(self):\n",
    "        self.x = 1\n",
    "        self.y = 0\n",
    "        self.memory = 0\n",
    "        self.signal = 0\n",
    "        self.done  = False\n",
    "        self.signalRewardIssued = False\n",
    "    \n",
    "        if np.random.randint(2):\n",
    "            self.rewardPosition  = self.altarOnePosition\n",
    "            self.penaltyPosition = self.altarTwoPosition\n",
    "            self.signal = 1\n",
    "        else:\n",
    "            self.rewardPosition  = self.altarTwoPosition\n",
    "            self.penaltyPosition = self.altarOnePosition\n",
    "            self.signal = 2\n",
    "        \n",
    "        return self.getState()\n",
    "    \n",
    "    def step(self, a):\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if not self.done:\n",
    "            if   a == 0:  # try to move -x\n",
    "                if self.x > 0:\n",
    "                  self.x -= 1  \n",
    "            elif a == 1:  # try to move +x\n",
    "                if self.x < self.xDim - 1:\n",
    "                  self.x += 1 \n",
    "            elif a == 2:  # try to move -y\n",
    "                if self.y > 0:\n",
    "                  self.y -= 1  \n",
    "            elif a == 3:  # try to move +y\n",
    "                if self.y < self.yDim - 1:\n",
    "                  self.y += 1\n",
    "            elif a == 4:  # set memory to 1\n",
    "                  self.memory = 1\n",
    "            elif a == 5:  # set memory to 2\n",
    "                  self.memory = 2\n",
    "            elif a == 6 and self.memoryClearable:  # set memory to 0\n",
    "              self.memory = 0\n",
    "        \n",
    "            if (self.x, self.y) == self.rewardPosition:\n",
    "                reward = self.altarReward\n",
    "                self.done = True\n",
    "                \n",
    "            elif (self.x, self.y) == self.penaltyPosition:\n",
    "                reward = self.altarPenalty\n",
    "                self.done = True\n",
    "                \n",
    "            elif (self.x, self.y) == self.signalPosition and not self.signalRewardIssued:\n",
    "                self.signalRewardIssued = True\n",
    "                reward = self.signalReward\n",
    "        \n",
    "        return (self.getState(), # state\n",
    "                reward,          # reward\n",
    "                self.done)       # done?\n",
    "    \n",
    "    renderSymbols = {0:'', 1:'*'}\n",
    "    \n",
    "    def render(self):\n",
    "        theMap = np.chararray([self.xDim,self.yDim])\n",
    "        theMap[:] = ' '\n",
    "       \n",
    "        theMap[self.rewardPosition] = '+'\n",
    "        theMap[self.penaltyPosition] = '-'\n",
    "        if self.signal == 1:\n",
    "          theMap[self.signalPosition] = '<'\n",
    "        else:\n",
    "          theMap[self.signalPosition] = '>'\n",
    "        \n",
    "        theMap[self.x,self.y] = '*'\n",
    "        \n",
    "        print(' ' + ''.join(['---' for i in range(self.xDim)]), end='\\n')\n",
    "        for row in theMap.transpose():\n",
    "           print('|', end='')\n",
    "           for c in row.decode(\"utf-8\"):\n",
    "              print(' ' + c + ' ', end='')\n",
    "           print('|\\n', end='')\n",
    "        print(' ' + ''.join(['---' for i in range(self.xDim)]), end='')\n",
    "        print('  memory: ' + str(self.memory))\n",
    "        #   print(''.join(row.decode(\"utf-8\")) )\n",
    "        \n",
    "a = temple()\n",
    "a.reset()\n",
    "print(a.step(1))\n",
    "a.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = temple()\n",
    "\n",
    "Q = np.zeros(env.observationSpace + env.actionSpace)\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 5000\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "\n",
    "    ## \"y': future reward for next state \"s1\"   \n",
    "\n",
    "    for j in range(99):\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "       # a = np.argmax(Q[s] + np.random.randn(1,env.actionSpace[0])*(10./(i+1)))\n",
    "        a = np.argmax(Q[s] + np.random.randn(1,env.actionSpace[0])*0.2 * np.argmax(Q[s]))\n",
    "\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d = env.step(a) # state, reward, done?\n",
    "\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s + tuple([a])] = Q[s + tuple([a])] + lr*(r + y*np.max(Q[s1,:]) - Q[s + tuple([a])])\n",
    "\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---------\n",
      "|    *    |\n",
      "|    >    |\n",
      "| -     + |\n",
      " ---------  memory: 0\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---------\n",
      "|         |\n",
      "|    >    |\n",
      "| *     + |\n",
      " ---------  memory: 1\n"
     ]
    }
   ],
   "source": [
    "a = np.argmax(Q[s] + np.random.randn(1,env.actionSpace[0])*(1./(i+1)))\n",
    "env.step(a)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1040,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
